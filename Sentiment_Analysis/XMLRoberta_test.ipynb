{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "MODEL = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "MAX_TRAINING_EXAMPLES = -1 # set this to -1 if you want to use the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Multiple_sources/processed_train.csv').dropna()\n",
    "x_train = train['text']\n",
    "y_train, _ = train['label'].factorize()\n",
    "test = pd.read_csv('./Multiple_sources/processed_test.csv').dropna()\n",
    "x_test = test['text']\n",
    "y_test, _ = test['label'].factorize()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374539    iÃ§indeki zil kedimin epey topun pesinden gidip...\n",
       "383411    Ã¼rÃ¼n bugÃ¼n elime ulaÅŸtÄ± french bulldog cinsi k...\n",
       "215734                      beÄŸendim ambalajlÄ± geldi tÅŸkler\n",
       "432730    taÅŸ kaÄŸÄ±t makasa forma kaybeden maldÄ±r amk hay...\n",
       "316221    KÃ¶yÃ¼n yerleÅŸime aÃ§Ä±k BahÃ§ecik adÄ±nda bir mezra...\n",
       "                                ...                        \n",
       "141501    Sezonun baÅŸÄ±nda Åampiyonlar Ligi eleme maÃ§larÄ±...\n",
       "226933    Ruhsal portreler kiÅŸilerin gÃ¶rÃ¼lebilen Ã¶zellik...\n",
       "232308             Ã¼rÃ¼n gÃ¼zel kurulumu kolay tavsiye ederim\n",
       "284265    Eski yapÄ±larÄ±n bulunduÄŸu alan Kent MeydanÄ± ola...\n",
       "399420                            kesinlikle tavsiye ederim\n",
       "Name: text, Length: 396576, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 0] = 3\n",
    "y_train[y_train == 1] = 0\n",
    "y_train[y_train == 3] = 1\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              Kral akbaba dikkat Ã§ekici renklere sahiptir\n",
       "1        Ä±srarla korkutmayÄ± baÅŸarÄ±yor korku uzun bitmiy...\n",
       "2        NeÅŸe ÃœzÃ¼ntÃ¼ kÃ¶prÃ¼nÃ¼n kÄ±rÄ±lmaya baÅŸlamasÄ±yla ge...\n",
       "3        i phone 5 ten sonra gene 4 ekranÄ± tercih ettim...\n",
       "4            BeÅŸinci sezonda diziye yeni oyuncular katÄ±ldÄ±\n",
       "                               ...                        \n",
       "48960    Fransa bayraÄŸÄ± diÄŸer kÄ±rmÄ±zÄ± zeminden beyaz bi...\n",
       "48961    Yine aynÄ± yÄ±l tÃ¼rkÃ¼ dalÄ±nda Murat Ã‡obanoÄŸlu bi...\n",
       "48962                             Kurgunu skiyim oÃ§ evladÄ±\n",
       "48963    ÅarkÄ± sonrasÄ±nda Damian Marley tarafÄ±ndan sesl...\n",
       "48964    berrak bir Ã¼rÃ¼n ancak kendi orijinal spigen ly...\n",
       "Name: text, Length: 48960, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=128))\n",
    "x_val = x_val.apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=128)) \n",
    "x_test = x_test.apply(lambda x: tokenizer(x, truncation=True, padding=\"max_length\", max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [input_ids, attention_mask]\n",
       "1        [input_ids, attention_mask]\n",
       "2        [input_ids, attention_mask]\n",
       "3        [input_ids, attention_mask]\n",
       "4        [input_ids, attention_mask]\n",
       "                    ...             \n",
       "44059    [input_ids, attention_mask]\n",
       "44060    [input_ids, attention_mask]\n",
       "44061    [input_ids, attention_mask]\n",
       "44062    [input_ids, attention_mask]\n",
       "44063    [input_ids, attention_mask]\n",
       "Name: text, Length: 44064, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.reset_index(drop=True, inplace=True)\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_tok = 0\n",
    "for i in x_train:\n",
    "    if len(i['input_ids']) > max_tok:\n",
    "        max_tok = len(i['input_ids'])\n",
    "max_tok\n",
    "3549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_tok = 0\n",
    "for i in x_val:\n",
    "    if len(i['input_ids']) > max_tok:\n",
    "        max_tok = len(i['input_ids'])\n",
    "max_tok\n",
    "2241"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_tok = 0\n",
    "for i in x_test:\n",
    "    if len(i['input_ids']) > max_tok:\n",
    "        max_tok = len(i['input_ids'])\n",
    "max_tok\n",
    "1331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\": torch.tensor(self.encodings[idx][\"input_ids\"])}\n",
    "        item['attention_mask'] = torch.tensor(self.encodings[idx][\"attention_mask\"])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "val_dataset = MyDataset(x_val, y_val)\n",
    "test_dataset = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(x_train.to_list(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(x_val.to_list(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(x_test.to_list(), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, y_train)\n",
    "val_dataset = MyDataset(val_encodings, y_val)\n",
    "test_dataset = MyDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=EPOCHS,                  # total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    logging_dir='./logs',                     # directory for storing logs\n",
    "    logging_steps=10,                         # when to print log\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                              # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                       # training arguments, defined above\n",
    "    train_dataset=train_dataset,              # training dataset\n",
    "    eval_dataset=val_dataset                  # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 44064\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.959     0.972     0.965     23592\n",
      "           1      0.990     0.998     0.994     15382\n",
      "           2      0.883     0.809     0.844      5090\n",
      "\n",
      "    accuracy                          0.962     44064\n",
      "   macro avg      0.944     0.926     0.934     44064\n",
      "weighted avg      0.961     0.962     0.961     44064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Val set has not been used yet. Test set gives similar result.(Add Test result too when running again.) \n",
    "val_preds_raw, val_labels , _ = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_preds_raw, axis=-1)\n",
    "print(classification_report(val_labels, val_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/best_model\n",
      "Configuration saved in ./results/best_model\\config.json\n",
      "Model weights saved in ./results/best_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./results/best_model\") # save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 14\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.7639523 ,  6.209841  , -3.5093257 ],\n",
       "       [ 3.3379538 , -4.2822948 , -0.07614036],\n",
       "       [-2.6974714 ,  6.151871  , -3.5292373 ],\n",
       "       [ 1.5008385 , -2.9210513 ,  0.7395926 ],\n",
       "       [ 2.81848   , -4.006406  ,  0.19193465],\n",
       "       [-0.7852182 ,  3.0212512 , -2.063731  ],\n",
       "       [-0.55728257, -3.1341426 ,  4.6073575 ],\n",
       "       [ 0.01268318,  0.98467207, -0.9847228 ],\n",
       "       [-0.84086907, -1.6498721 ,  3.7046628 ],\n",
       "       [-1.0766885 ,  3.684284  , -2.416834  ],\n",
       "       [-0.43949133, -3.2303922 ,  4.5779424 ],\n",
       "       [-0.7951734 ,  2.7179723 , -1.6944032 ],\n",
       "       [-0.79751134,  3.0040443 , -2.0193284 ],\n",
       "       [ 4.650012  , -3.9620986 , -1.5205253 ]], dtype=float32), label_ids=array([1, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0], dtype=int64), metrics={'test_loss': 1.9274097681045532, 'test_runtime': 0.2101, 'test_samples_per_second': 66.649, 'test_steps_per_second': 4.761})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_test = pd.Series(['KurtuluÅŸ SavaÅŸÄ± Ä°zmir in isgaliyle baÅŸlar, Ä°zmirin kurtuluÅŸu ile biter.',\n",
    "                        'SeÃ§imi kaybedeceÄŸini bilen bir hÃ¼kÃ¼met, \"30 gÃ¼n prim Ã¶deyen herkes, yaÅŸÄ±na bakÄ±lmaksÄ±zÄ±n ve derhal kiÅŸi baÅŸÄ±na dÃ¼ÅŸen gelir kadar emekli aylÄ±ÄŸÄ± almaya hak kazanÄ±r\" diye bir kanun Ã§Ä±karsa, bu kanun bir kere Ã§Ä±ktÄ±, kazanÄ±lmÄ±ÅŸ hak oldu, herkese bu Ã¶deme yapÄ±lsÄ±n diyecek misin?',\n",
    "                        'KarÄ±nca kolonisi optimizasyonu her zaman iyi sonuÃ§ vermeyebilir.',\n",
    "                        'Siyasilere manifesto: Benim paramla ulufe daÄŸÄ±tma yarÄ±ÅŸÄ±na giresiniz diye vergi Ã¶demiyorum.',\n",
    "                        'Ben ekonomideki cehaletin boyutlarÄ±nÄ± gÃ¶rÃ¼nce artÄ±k yapÄ±cÄ± yÄ±kÄ±mdan baÅŸka bir yol olmadÄ±ÄŸÄ±nÄ± anlÄ±yorum. Maliyeti engellemek iÃ§in tÃ¼m samimiyetimizle doÄŸruyu sÃ¶yledik ama artÄ±k popÃ¼lizm tavan yapmÄ±ÅŸ durumda. Hepimiz Ã¶deyeceÄŸiz bu maliyeti!',\n",
    "                        'DÃ¼nyanÄ±n en bÃ¼yÃ¼k Ã¼Ã§Ã¼ncÃ¼ ekonomisini en uzun sÃ¼re yÃ¶netmiÅŸ adam suikaste uÄŸruyor ancak bu olay medyada Ã§ok az yer alÄ±yor. MaktÃ¼l bu kadar bÃ¼yÃ¼kken bu kadar cÄ±lÄ±z ses Ã§Ä±kmasÄ± ÅŸaÅŸÄ±rtÄ±cÄ±.',\n",
    "                        'Seni it oÄŸlu kÃ¶pek ÅŸerefsiz.',\n",
    "                        'Bunu tasvip etmiyorum.',\n",
    "                        'Ã‡ok kÃ¶tÃ¼ bir geliÅŸme.',\n",
    "                        'Bu hiÃ§te kÃ¶tÃ¼ deÄŸil.',\n",
    "                        'Rezalet.',\n",
    "                        'Hisse artÄ±ÅŸa geÃ§ti Ã§abuk alÄ±n.',\n",
    "                        'Olley be bist100 uÃ§tu.',\n",
    "                        'Bist100 tavsiye ederim kaÃ§Ä±rmayÄ±n.'])\n",
    "online_test = online_test.apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=128))\n",
    "online_ds = MyDataset(online_test, [1, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0])\n",
    "trainer.predict(online_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 7\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 2.6527088 , -3.9112282 ,  0.28650457],\n",
       "       [ 2.181105  , -3.6231873 ,  0.5682687 ],\n",
       "       [ 0.96109605, -1.7998203 ,  0.33854768],\n",
       "       [-0.6710125 ,  2.2690775 , -1.4880042 ],\n",
       "       [ 0.840981  , -3.963299  ,  3.5903444 ],\n",
       "       [ 3.4300041 , -4.3258862 , -0.12243924],\n",
       "       [-0.94187367,  3.4066384 , -2.2870247 ]], dtype=float32), label_ids=array([0, 0, 0, 2, 0, 0, 0], dtype=int64), metrics={'test_loss': 1.6829805374145508, 'test_runtime': 0.186, 'test_samples_per_second': 37.626, 'test_steps_per_second': 5.375})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_test = pd.Series(['ArkadaÅŸlar 1 yÄ±ldÄ±r deÄŸiÅŸmeyen takip listemin baÅŸarÄ± oranÄ±m/ 100 durBÃ¶yle olunca aynÄ±sÄ±nÄ± deÄŸerli ğŸ˜€ğŸ˜€ğŸ˜€trader...kopyala yapÄ±ÅŸtÄ±r misali takip listeleri olmuÅŸ  birebirğŸ˜€ğŸ˜€ AyÄ±ptÄ±r emektir ustanÄ±n biraz edep yahu edep.',\n",
    "                        'Ä°ki aya kalmaz yÃ¼zde %400 gidecek iki #hisse gelecek #bist100 500 rt gelecek mi #bist30',\n",
    "                        '#Bist100 #xu100 dolar bazlÄ±; yeÅŸil Ã§izginin Ã¼stÃ¼nde kaldÄ±, yÃ¶nÃ¼nÃ¼ yukarÄ± Ã§evirdi. Pembe Ã§izgiyi boÅŸlukla aÅŸacaÄŸÄ±, son yÄ±llarÄ±n en gÃ¼Ã§lÃ¼ rallisinin eÅŸiÄŸinde olabilir. KÄ±sa vadeli hedefi 273. Orta vadeli hedefi 324. YatÄ±rÄ±m tavsiyesi deÄŸildir. #Elliott',\n",
    "                        'Bu hafta, yatÄ±rÄ±m araÃ§larÄ±ndan #borsa yatÄ±rÄ±mcÄ±ya kaybettirirken, #altÄ±n ve #dolar kazandÄ±rdÄ±.Hafta iÃ§inde en dÃ¼ÅŸÃ¼k 3.048,67 puanÄ±, en yÃ¼ksek 3.319,14 puanÄ± gÃ¶ren #bist100 endeksi haftayÄ± %3,10 kayÄ±pla 3.179,99 puandan tamamladÄ±.',\n",
    "                        '#BORSA Ã‡enem dÃ¼ÅŸtÃ¼ azÄ±cÄ±k konuÅŸtum ister izleyin ister izlemeyin Sonra gidip uÃ§an balonlara binin',\n",
    "                        'BugÃ¼n akÅŸam size pazartesinin tavanÄ±nÄ± yazacaÄŸÄ±m.Tavan hissem iÃ§in bol BEÄENÄ° ve RT ile destek verin.Yeni ğŸ”’ KÄ°LÄ°T hissemi bu akÅŸam paylaÅŸacaÄŸÄ±mğŸš€ğŸš€ğŸš€ğŸš€ğŸ”’ğŸ”’ğŸ”’ğŸ”’#borsa #bist100 #tavan #rally',\n",
    "                        'YÃ¼kseliÅŸ trendine geÃ§erek ilk direnÃ§ 30.62 seviyelerini deneyip tekrar dÃ¼ÅŸÃ¼ÅŸ yaÅŸayan #EREGL yatay olarak seyir gÃ¶stermektedir.  Tekrar dÃ¼ÅŸÃ¼ÅŸ yaÅŸamasÄ± durumunda 29.96 ve 29.66 destek seviyelerini test etmesi muhtemeldir.'])\n",
    "online_test = online_test.apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=128))\n",
    "online_ds = MyDataset(online_test, [0, 0, 0, 2, 0, 0, 0])\n",
    "trainer.predict(online_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
